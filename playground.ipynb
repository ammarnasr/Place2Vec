{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_handler import  get_data, get_gdf\n",
    "\n",
    "df = get_data()\n",
    "display(df.head())\n",
    "\n",
    "print(f'Number of Groups: {df.group_code.nunique()}')\n",
    "print(f'Number of Categories: {df.category_code.nunique()}')\n",
    "print(f'Number of Classes: {df.pointx_class_code.nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "pointx_class = df['pointx_class']\n",
    "pointx_class_counts = pointx_class.value_counts()\n",
    "pointx_class_counts = pointx_class_counts.sort_values(ascending=False)\n",
    "x = list(range(1, len(pointx_class_counts) + 1))\n",
    "y1 = pointx_class_counts\n",
    "y2 = np.log(pointx_class_counts)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(20, 8))\n",
    "\n",
    "sns.scatterplot(x=x, y=y1, marker='o', ax=axs[0], s=150)\n",
    "sns.scatterplot(x=x, y=y2, marker='o', ax=axs[1], s=150)\n",
    "axs[0].set_xlabel('Rank')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Rank vs. Frequency of pointx_class')\n",
    "axs[0].grid(True)\n",
    "axs[1].set_xlabel('Rank')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Rank vs. Frequency of pointx_class Log Scale')\n",
    "axs[1].grid(True)\n",
    "\n",
    "for i in range(4):\n",
    "    axs[0].arrow(x[i]          , y1[i], (i+1)*40, 0, head_width=3, color='black')\n",
    "    axs[0].text((i+1)*40 + x[i], y1[i], y1.index[i], fontsize=14) \n",
    "    axs[1].arrow(x[i]          , y2[i], (i+1)*40, 0, head_width=0.08, color='black')\n",
    "    axs[1].text((i+1)*40 + x[i], y2[i], y2.index[i], fontsize=14) \n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Generate random points with a minimal interval to construct POI sequences in Exeter. \n",
    "\n",
    "- These random points are generated along with the road network (excluding unclassified roads).\n",
    "- The distance interval between any two random points is 50 metres.\n",
    "- These constraints produce a structured set of random points R {r1, r2, ..., ri, ..., rn}\n",
    "- The maximum size possible for the set R is N.\n",
    "- For each random point ri, we search its accessible POIs within 200 metres and thus obtain an accessible POI set Si. \n",
    "- Compute the distance for all of the pairs between the central random point and POIs within Si.\n",
    "- Use the distance as a reference by which build a sequentially ordered POI list Li = [poi1, poi2, poi3, ..poin],where i refers to the index of the random point, n is the number of accessible POIs around the random point ri and elements in the list refer to the POI classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import get_poi_sequences\n",
    "from vis import full_sequence_map, sample_sequence_map\n",
    "\n",
    "full_sequence_gdf = get_poi_sequences()\n",
    "full_sequence_map(full_sequence_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "\n",
    "dl = get_dataloader(context_length=128, bs=16, shuffle=True)\n",
    "x, y = next(iter(dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from doc2vec import  main, save_checkpoint\n",
    "pois_dataset_args = {'force_recreate': False, 'max_sequence_size': 200}\n",
    "dataloader_args = {'batch_size': 1024, 'num_workers': 0, 'shuffle': True, 'drop_last': True}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 10000\n",
    "lr = 0.01\n",
    "min_delta = None\n",
    "vec_dim = 20\n",
    "vocab_min_count=0\n",
    "n_negative_samples=5\n",
    "context_size=10\n",
    "concat = False\n",
    "\n",
    "model, training_losses, vocab, ds = main(\n",
    "    dataset_args=pois_dataset_args, dataloader_args=dataloader_args, concat=concat,\n",
    "    vec_dim = vec_dim,  vocab_min_count=vocab_min_count, n_negative_samples=n_negative_samples, \n",
    "    context_size=context_size, device=device, epochs=epochs, lr=lr, min_delta=min_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path =  f'models/doc2vec_checkpoint.pt'\n",
    "ckpt_path = save_checkpoint(model=model, training_losses=training_losses, vocab=vocab, ds=ds, filename=checkpoint_path, add_timestamp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doc2vec import  load_checkpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "#select the latest checkpoint\n",
    "ckpt_path = max([f'models/{f}' for f in os.listdir('models') if f.startswith('doc2vec_checkpoint')])\n",
    "print(ckpt_path)\n",
    "\n",
    "model, training_losses, vocab, ds = load_checkpoint(ckpt_path)\n",
    "\n",
    "seqs = []\n",
    "paragraphs = []\n",
    "seq_ids = []\n",
    "for item in ds:\n",
    "    seqs.append(item['seq'])\n",
    "    paragraphs.append(item['paragraph'])\n",
    "    seq_ids.append(item['seq_id'])\n",
    "\n",
    "seqs = pd.DataFrame({'seq': seqs, 'paragraph': paragraphs, 'seq_id': seq_ids})\n",
    "\n",
    "paragraph_embeddings = model.paragraph_matrix.to('cpu').detach().numpy()\n",
    "word_embeddings = model.word_matrix.to('cpu').detach().numpy()\n",
    "words = vocab.words\n",
    "paragraphs = vocab.paragraphs\n",
    "words2idx = vocab.word2idx\n",
    "paragraphs2idx = vocab.paragraph2idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_sim = cosine_similarity(word_embeddings, word_embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.heatmap(word_sim, ax=ax, cmap='RdBu', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "n_clusters = 9\n",
    "linkage = 'complete'\n",
    "metric = 'cosine'\n",
    "compute_full_tree = True\n",
    "compute_distances = True\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage, metric=metric, \n",
    "                                compute_full_tree=compute_full_tree, compute_distances=compute_distances)\n",
    "\n",
    "model = model.fit(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
